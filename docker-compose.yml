# BEND/docker-compose.yml

networks:
  bend-net:
    driver: bridge

services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    restart: unless-stopped
    ports:
      - "${OPENWEBUI_PORT:-12002}:3000"
    environment:
      # Point the UI to the main reasoning model for general chat
      - OLLAMA_API_BASE_URL=http://ollama-reasoning:11434
      - WEBUI_URL=http://localhost:12002
      - PORT=3000
      - ENABLE_PERSISTENT_CONFIG=true
    volumes:
      - openwebui_data:/app/backend/data
    networks:
      - bend-net
    depends_on:
      - ollama-reasoning

  whisper:
    build:
      context: .
      dockerfile: Dockerfile.build-services
      target: whisper-builder
      args:
        - GPU_ENABLED=${WHISPER_GPU_ENABLED:-false}
    container_name: whisper
    restart: unless-stopped
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=${TIMEZONE:-America/New_York}
      - WHISPER_MODEL=${WHISPER_MODEL:-small.en}
    command: >
      /app/build/bin/whisper-server
      --model /app/models/ggml-${WHISPER_MODEL}.bin
      --threads 4
      --host 0.0.0.0
      --port 9000
      --convert
      --flash-attn
    volumes:
      - whisper_data:/config
    ports:
      - "${WHISPER_PORT:-12003}:9000"
    networks:
      - bend-net

  piper:
    build:
      context: .
      dockerfile: Dockerfile.piper
    container_name: piper
    restart: unless-stopped
    environment:
      - LD_LIBRARY_PATH=./piper
    command: >
      python3 -m piper.http_server
      --model /app/${PIPER_VOICE:-en_US-lessac-medium}.onnx
      --port 59125
      --host 0.0.0.0
    ports:
      - "${PIPER_PORT:-12004}:59125"
    networks:
      - bend-net

  glances:
    image: nicolargo/glances:latest
    container_name: glances
    restart: unless-stopped
    ports:
      - "${GLANCES_PORT:-12005}:61208"
    pid: "host"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - GLANCES_OPT=-w
    networks:
      - bend-net

  qdrant:
    image: qdrant/qdrant
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "${QDRANT_PORT:-12006}:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - bend-net

  retriever:
    build:
      context: .
      dockerfile: ./rag/Dockerfile
    container_name: retriever
    restart: unless-stopped
    ports:
      - "${RETRIEVER_PORT:-12007}:8000"
    depends_on:
      - qdrant
    volumes:
      - ./rag/docs:/app/docs
    environment:
      - BACKEND_API_KEY=${BACKEND_API_KEY}
      - OTEL_SERVICE_NAME=retriever
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT}
    networks:
      - bend-net

  voiceproxy:
    build:
      context: .
      dockerfile: ./voice-proxy/Dockerfile
    container_name: voiceproxy
    restart: unless-stopped
    ports:
      - "${VOICEPROXY_PORT:-12008}:8001"
    depends_on:
      - whisper
      - piper
    environment:
      - BACKEND_API_KEY=${BACKEND_API_KEY}
      - OTEL_SERVICE_NAME=voiceproxy
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT}
    networks:
      - bend-net

  # === MoE Ollama Specialist: Reasoning ===
  ollama-reasoning:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama-reasoning
    restart: unless-stopped
    ports:
      - "${OLLAMA_REASONING_PORT:-12009}:11434"
    volumes:
      - ./scripts/entrypoint-ollama.sh:/entrypoint-ollama.sh
      - ollama_reasoning_data:/root/.ollama
    environment:
      - OLLAMA_PULL_MODEL=${OLLAMA_REASONING_PULL_MODEL}
    entrypoint: /entrypoint-ollama.sh
    networks:
      - bend-net

  redis:
    image: redis/redis-stack:latest
    container_name: redis
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-12010}:6379"
    networks:
      - bend-net

  nemoguardrails:
    build:
      context: .
      dockerfile: Dockerfile.nemo
    container_name: nemoguardrails
    restart: unless-stopped
    command: ["nemoguardrails", "server", "--config=./config", "--default-config-id", "config"]
    ports:
      - "${GUARDRAILS_PORT:-12012}:8000"
    networks:
      - bend-net

  # === MoE Ollama Specialist: Agentic/RAG ===
  ollama-agentic:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama-agentic
    restart: unless-stopped
    ports:
      - "${OLLAMA_AGENTIC_PORT:-12013}:11434"
    volumes:
      - ./scripts/entrypoint-ollama.sh:/entrypoint-ollama.sh
      - ollama_agentic_data:/root/.ollama
    environment:
      - OLLAMA_PULL_MODEL=${OLLAMA_AGENTIC_PULL_MODEL}
    entrypoint: /entrypoint-ollama.sh
    networks:
      - bend-net

  # === MoE Ollama Specialist: Coding/Math ===
  ollama-coding:
    build:
      context: .
      dockerfile: Dockerfile.ollama
    container_name: ollama-coding
    restart: unless-stopped
    ports:
      - "${OLLAMA_CODING_PORT:-12014}:11434"
    volumes:
      - ./scripts/entrypoint-ollama.sh:/entrypoint-ollama.sh
      - ollama_coding_data:/root/.ollama
    environment:
      - OLLAMA_PULL_MODEL=${OLLAMA_CODING_PULL_MODEL}
    entrypoint: /entrypoint-ollama.sh
    networks:
      - bend-net

  # === MoE Router Service ===
  moe-router:
    build:
      context: .
      dockerfile: ./moe-router/Dockerfile
    container_name: moe-router
    restart: unless-stopped
    ports:
      - "${MOE_ROUTER_PORT:-12016}:8000"
    depends_on:
      - ollama-reasoning
      - ollama-agentic
      - ollama-coding
    networks:
      - bend-net

volumes:
  qdrant_data:
  openwebui_data:
  whisper_data:
  ollama_reasoning_data:
  ollama_agentic_data:
  ollama_coding_data: