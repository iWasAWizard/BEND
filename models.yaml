# BEND/models.yaml
# models.yaml â€” Consolidated model registry for BEND and AEGIS

models:
  # === Models for vLLM (Primary) and KoboldCPP (GGUF Fallback) ===
  - key: "hermes"
    # The 'name' field is the Hugging Face repo ID for vLLM
    name: "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"
    # The 'backend_model_name' is the specific .gguf filename for KoboldCPP
    backend_model_name: "Nous-Hermes-2-Mistral-7B-DPO-GGUF.Q4_K_M.gguf"
    filename_pattern: ["nous-hermes-2-mistral-7b"]
    formatter_hint: "chatml" # Hermes models typically use ChatML
    default_max_context_length: 16384
    quant_example: "Q4_K_M"
    use_case: "General-purpose assistant, strong reasoning"
    url: "https://huggingface.co/TheBloke/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/nous-hermes-2-mistral-7b.Q4_K_M.gguf"
    notes: "Primary model for BEND. Set via switch-model.sh."

  - key: "mythomax"
    name: "Gryphe/MythoMax-L2-13b"
    backend_model_name: "mythomax-l2-13b.Q5_K_M.gguf"
    filename_pattern: ["mythomax-l2-13b"]
    formatter_hint: "alpaca" # MythoMax is often fine-tuned on Alpaca format
    default_max_context_length: 8192
    quant_example: "Q5_K_M"
    use_case: "Roleplay, lore-heavy chat"
    url: "https://huggingface.co/TheBloke/MythoMax-L2-13B-GGUF/resolve/main/mythomax-l2-13b.Q4_K_M.gguf"

  - key: "mistral-openorca"
    name: "Open-Orca/Mistral-7B-OpenOrca"
    backend_model_name: "mistral-7b-openorca.Q4_K_M.gguf"
    filename_pattern: ["mistral-7b-openorca"]
    formatter_hint: "chatml" # OpenOrca datasets often use ChatML
    default_max_context_length: 8192
    quant_example: "Q4_K_M"
    use_case: "Fast general chat, coding"
    url: "https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-GGUF/resolve/main/mistral-7b-openorca.Q4_K_M.gguf"

  - key: "deepseek-coder"
    name: "deepseek-ai/deepseek-coder-33b-instruct"
    backend_model_name: "deepseek-coder-33b.Q4_K_M.gguf"
    filename_pattern: ["deepseek-coder-33b"]
    formatter_hint: "codellama-instruct" # Deepseek Coder uses a similar format
    default_max_context_length: 16384
    quant_example: "Q4_K_M"
    use_case: "Software dev, multi-file context"
    url: "https://huggingface.co/TheBloke/deepseek-coder-33b-GGUF/resolve/main/deepseek-coder-33b.Q4_K_M.gguf"

  - key: "kobold-llama3-gguf"
    name: "meta-llama/Meta-Llama-3-8B-Instruct"
    backend_model_name: "Llama-3-8B-Instruct-IQ4_XS.gguf"
    filename_pattern: ["Llama-3-8B-Instruct-IQ4_XS.gguf", "Llama-3-8B-Instruct"]
    formatter_hint: "llama3"
    default_max_context_length: 4096
    quant_example: "IQ4_XS"
    use_case: "General-purpose assistant, strong reasoning"
    url: "https://huggingface.co/QuantFactory/Llama-3-8B-Instruct-GGUF/resolve/main/Llama-3-8B-Instruct-IQ4_XS.gguf"
    notes: "For use with KoboldCPP. Assumes KOBOLDCPP_MODEL env var points to this GGUF."```