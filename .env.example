# BEND/.env.example
# This file is managed by ./scripts/switch-model.sh
# You can add your Hugging Face token here for downloading gated models.

# --- HUGGING FACE AUTH ---
# Required for downloading models like Llama 3
HF_TOKEN=""

# --- CORE MODEL CONFIG ---
# The Hugging Face repo ID for the model to be served by vLLM.
MODEL_NAME="NousResearch/Nous-Hermes-2-Mistral-7B-DPO"
# The specific .gguf filename for the model to be served by KoboldCPP.
KOBOLDCPP_MODEL_NAME="Nous-Hermes-2-Mistral-7B-DPO-GGUF.Q4_K_M.gguf"
# The context size (in tokens) for the model.
MODEL_CONTEXT_SIZE=16384

# --- SERVICE CONFIG ---
# URL for OpenWebUI to connect to the LLM backend.
OLLM_API_BASE_URL=http://vllm:8000/v1
# Optional API key to secure BEND's internal services (retriever, voiceproxy).
BEND_API_KEY=""
# Select the voice model for Piper TTS.
PIPER_VOICE_MODEL=en_US-lessac-medium
# Whisper model to use (e.g., "small.en", "large-v2").
WHISPER_MODEL=small.en


# --- GPU CONFIG ---
# Number of GPU layers to offload to the GPU for KoboldCPP. 0 for CPU-only.
KOBOLD_GPU_LAYERS=0
# GPU memory utilization for vLLM (0.0 to 1.0).
VLLM_GPU_MEMORY_UTILIZATION=0.90

# --- PORT MAPPINGS ---
# You can change these if the default ports conflict with other services.
OPENWEBUI_PORT=12002
WHISPER_PORT=12003
PIPER_PORT=12004
GLANCES_PORT=12005
QDRANT_PORT=12006
RETRIEVER_PORT=12007
VOICEPROXY_PORT=12008
OLLAMA_PORT=12009
REDIS_PORT=12010
VLLM_PORT=12011
GUARDRAILS_PORT=12012