# BEND/.env.example
# This file is managed by ./scripts/switch-model.sh
# You can add your Hugging Face token here for downloading gated models.

# --- HUGGING FACE AUTH ---
# Required for downloading models like Llama 3
HF_TOKEN=""

# --- CORE MODEL CONFIG ---
# The Hugging Face repo ID for the model to be served by vLLM.
MODEL_NAME="ibm-granite/granite-3.3-8b-instruct"
# The context size (in tokens) for the model.
MODEL_CONTEXT_SIZE=16384
# The model name for the Ollama service to automatically pull (e.g., 'llama3:8b-instruct').
OLLAMA_PULL_MODEL=granite-code:3b

# --- SERVICE CONFIG ---
# Comma-separated list of backend URLs for OpenWebUI.
OLLM_API_BASE_URL=http://vllm:8000,http://ollama:11434
# Whisper model to use (e.g., "small.en", "large-v2").
WHISPER_MODEL=small.en
# Select the voice model for Piper TTS.
PIPER_VOICE_MODEL=en_US-lessac-medium
# Optional API key to secure BEND's internal services (retriever, voiceproxy).
BACKEND_API_KEY=""
# OpenTelemetry Exporter Endpoint. Set to a collector URL (e.g., http://jaeger:4317)
# to enable tracing. Leave blank to disable.
OTEL_EXPORTER_OTLP_ENDPOINT=""

# --- GPU CONFIG ---
# GPU memory utilization for vLLM (0.0 to 1.0).
VLLM_GPU_MEMORY_UTILIZATION=0.90

# --- PORT MAPPINGS ---
# You can change these if the default ports conflict with other services.
OPENWEBUI_PORT=12002
WHISPER_PORT=12003
PIPER_PORT=12004
GLANCES_PORT=12005
QDRANT_PORT=12006
RETRIEVER_PORT=12007
VOICEPROXY_PORT=12008
OLLAMA_PORT=12009
REDIS_PORT=12010
VLLM_PORT=12011
GUARDRAILS_PORT=12012